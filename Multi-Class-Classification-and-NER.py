# -*- coding: utf-8 -*-
"""KitapBolumuDogalDil_24112024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zYN31UpHWTFGzSR4jzDmLa13s2rPAmJZ

# KitapBolumuDogalDil

**Notebook Structure:**

The notebook is structured as follows:

For two datasets, **PubMed 20k RCT** and **BC5CDR** (BioCreative V CDR corpus), different NLP tasks are applied: "Multi_class Classification" for PubMed 20k RCT and "Named Entity Recognition" (NER) for BC5CDR. The models are trained on two different pre_trained BERT models: **bert-base-cased** and **biobert-base-cased-v1.2** (a pre-trained biomedical language representation model specifically designed for biomedical text mining; more information can be found here:https://academic.oup.com/bioinformatics/article/36/4/1234/5566506 ).

**Differences Between the Two Datasets:**

    PubMed 20k RCT:
        A text classification dataset.
        Each abstract is split into sentences, and each sentence is labeled with a specific section type.
        Classes: BACKGROUND, OBJECTIVE, METHODS, RESULTS, or CONCLUSIONS.

    BC5CDR:
        A Named Entity Recognition (NER) dataset.
        Token-level annotations in BIO format are used to tag diseases and chemicals.
        In BIO format:
            B- indicates the beginning of an entity (e.g., B-Disease).
            I- indicates the inside of an entity (e.g., I-Disease).
            O indicates a token that is outside any entity.


**For each dataset, the notebook is divided into the following four sections:**

1.   Setting up the GPU Environment
2.   Getting Data
3.   Training and Testing the Model
4.   Using the Model (Running Inference)

**Technical Details_Fine-tuning Parameters:**

    num_train_epochs=1: Number of epochs (this can be increased to 3 for better results; however, due to GPU limitations and to reduce training time, 1 epoch was selected).
    train_batch_size=16: Batch size for training.
    eval_batch_size=16: Batch size for evaluation.

**General Results:**

For both tasks (multi-class classification and NER), models trained on biobert-base-cased-v1.2 demonstrated better performance compared to bert-base-cased:

*PubMed 20k RCT + Task: Multi-Class Classification*

    Model: bert-base-cased
        MCC: 0.8247
        Precision: 0.8777, Recall: 0.8694, F1: 0.8711
        
    Model: biobert-base-cased-v1.2
        MCC: 0.8290
        Precision: 0.8779, Recall: 0.8726, F1: 0.8736


*BC5CDR + Task: NER*

    Model: bert-base-cased
        Precision: 0.7883, Recall: 0.8569, F1: 0.8211


    Model: biobert-base-cased-v1.2
        Precision: 0.8398, Recall: 0.8939, F1: 0.8660

# Data: PubMed 20k RCT

•	Task: Multi_class classification

•	Paper: Franck Dernoncourt, Ji Young Lee. PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts. International Joint Conference on Natural Language Processing (IJCNLP). 2017.

•	Details: 20,000 abstracts of randomized controlled trials.

•	Labels(targets): Methods, Results, Conclusions, Background, Objective.

•	Data source: https://github.com/Franck-Dernoncourt/pubmed-rct/tree/master

**Section 1: Setting up the GPU Environment**
"""

# Install required libraries
!pip install transformers datasets torch scikit-learn --quiet
!pip install simpletransformers --quiet
import torch
import pandas as pd
import os
import logging
from simpletransformers.classification import ClassificationModel, ClassificationArgs
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_recall_fscore_support, accuracy_score

# Set up logging
logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

# Check for GPU

if torch.cuda.is_available():
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")
else:
    print("Using CPU")

"""**Section 2: Getting Data**


"""

# Clone the dataset
!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git
!ls pubmed-rct

"""The downloaded repository contains four folders, each with a version of the PubMed 200k RCT dataset. According to the README:



*   PubMed 20k is a subset of PubMed 200k.
*   The _numbers_replaced_with_at_sign versions replace all numbers with @.

   

We'll focus on PubMed_20k_RCT , a smaller subset ideal for quicker experiments.
"""

data_dir= "pubmed-rct/PubMed_20k_RCT/"

# Check all of the filenames in the target directory
filenames = [data_dir + filename for filename in os.listdir(data_dir)]
filenames

# Example of raw data

def get_lines(filepath):
    with open(filepath, "r") as file:
        return file.readlines()
train_lines = get_lines(data_dir+"train.txt")
train_lines[:3]

# Data preprocessing

def preprocess_text(filename):
    """Returns a list of dictionaries of abstract line data.

    Takes in a filename, reads its contents, and processes each line,
    extracting the target label and the text of the sentence.

    Args:
        filename: A string of the target text file to read and extract line data from.

    Returns:
        A list of dictionaries, each containing a line's label and text.
        Example:
        [{"target": 'CONCLUSION',
          "text": "the study couldn't have gone better, turns out people are kinder than you think"}]
    """


    input_lines = get_lines(filename)  # Read all lines from the file
    abstract_lines = ""  # Create an empty string for the abstract
    abstract_samples = []  # List to hold processed abstract samples

    # Loop through each line in the file
    for line in input_lines:
        if line.startswith("###"):  # Check if the line is an ID line
            abstract_lines = ""  # Reset the abstract string
        elif line.isspace():  # Check if the line is a new line
            abstract_line_split = abstract_lines.splitlines()  # Split the abstract into separate lines

            # Process each line in the abstract
            for abstract_line in abstract_line_split:
                line_data = {}  # Dictionary to store line data
                target_text_split = abstract_line.split("\t")  # Split label and text
                line_data["target"] = target_text_split[0]  # Extract the target label
                # line_data["text"] = target_text_split[1].lower()  # Extract the sentence text
                line_data["text"] = target_text_split[1]  # Extract the sentence text
                abstract_samples.append(line_data)  # Append to the list of abstract samples
        else:  # Otherwise, it's part of an abstract
            abstract_lines += line

    return abstract_samples

train_samples = preprocess_text("pubmed-rct/PubMed_20k_RCT/train.txt")
test_samples = preprocess_text("pubmed-rct/PubMed_20k_RCT/test.txt")
dev_samples = preprocess_text("pubmed-rct/PubMed_20k_RCT/dev.txt")

# Convert to pandas DataFrame for easier manipulation

train_df = pd.DataFrame(train_samples)
test_df = pd.DataFrame(test_samples)
dev_df = pd.DataFrame(dev_samples)

print("Example of preprocessd_train_data:\n",train_df.head())

#Check Dataset Sizes:
print("Training dataset size:", len(train_df))
print("Test dataset size:", len(test_df))
print("Dev dataset size:", len(dev_df))

# Ensure the columns are in the format SimpleTransformers expects
train_df = train_df.rename(columns={"text": "text", "target": "labels"})
test_df = test_df.rename(columns={"text": "text", "target": "labels"})
dev_df = dev_df.rename(columns={"text": "text", "target": "labels"})

# Optional: Encode labels to integers
label_encoder = LabelEncoder()
train_df["labels"] = label_encoder.fit_transform(train_df["labels"])
test_df["labels"] = label_encoder.transform(test_df["labels"])
dev_df["labels"] = label_encoder.transform(dev_df["labels"])
print(train_df.head())

"""**Section 3: Training and Testing the Model**"""

# Optional model configuration
model_args = ClassificationArgs(
    num_train_epochs=1,             # Number of epochs
    train_batch_size=16,            # Batch size for training
    eval_batch_size=16,             # Batch size for evaluation
    overwrite_output_dir=True,      # Overwrite the output directory
    output_dir="./ClassificationModel_bert",  # Output directory
)

# Create a ClassificationModel: Pre-trained BERT model="bert-base-cased"
model_bert = ClassificationModel(
    "bert",                         # Use the BERT architecture
    "bert-base-cased",            # Pre-trained BERT model
    num_labels=len(label_encoder.classes_),  # Number of labels
    use_cuda=torch.cuda.is_available(),  # True: Use GPU if available
    args=model_args                 # Model arguments
)

# Train the model
model_bert.train_model(train_df,eval_data=dev_df)

# Define a custom metrics function
def compute_metrics(preds, labels):
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    accuracy = accuracy_score(labels, preds)
    return {
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "accuracy": accuracy,
    }


# Evaluate the model
result, model_outputs, wrong_predictions = model_bert.eval_model(test_df, custom_metrics=compute_metrics)

# Print evaluation results
print("Evaluation Results on fine-tuned 'bert-base-cased':")
for metric, value in result.items():
    if isinstance(value, dict):
        print(f"{metric}:")
        for sub_metric, sub_value in value.items():
            print(f"  {sub_metric}: {sub_value:.4f}")
    elif isinstance(value, (int, float)):
        print(f"{metric}: {value:.4f}")
    else:
        print(f"{metric}: {value}")

# Create a ClassificationModel: Pre-trained BERT model="biobert-base-cased-v1.2"
model_biobert = ClassificationModel(
    "bert",                         # Use the BERT architecture
    "dmis-lab/biobert-base-cased-v1.2",            # Pre-trained BERT model
    num_labels=len(label_encoder.classes_),  # Number of labels
    use_cuda=torch.cuda.is_available(),  # True: Use GPU if available
    args=model_args                 # Model arguments
)

# Train the model
model_biobert.train_model(train_df,eval_data=dev_df)
# Evaluate the model
result, model_outputs, wrong_predictions = model_biobert.eval_model(test_df, custom_metrics=compute_metrics)

# Print evaluation results
print("Evaluation Results on fine-tuned 'biobert-base-cased-v1.2':")
for metric, value in result.items():
    if isinstance(value, dict):
        print(f"{metric}:")
        for sub_metric, sub_value in value.items():
            print(f"  {sub_metric}: {sub_value:.4f}")
    elif isinstance(value, (int, float)):
        print(f"{metric}: {value:.4f}")
    else:
        print(f"{metric}: {value}")

"""**Section 4: Using the Model**"""

# Make predictions
sample_texts = [" We prolonged the exposure period and investigated how quickly diesel exhaust can induce respiratory and systemic effects ."]
#correct lable is:BACKGROUND

## model_bert
print("### Predictions of fine-tuned 'bert-base-cased':\n")
predictions, raw_outputs = model_bert.predict(sample_texts)
# print("Predictions:", predictions)
# print("Raw Outputs:", raw_outputs)
# Decode label predictions
decoded_predictions = label_encoder.inverse_transform(predictions)
print("Decoded Predictions:", decoded_predictions)

## model_biobert
print("\n### Predictions of fine-tuned 'biobert-base-cased-v1.2':\n")
predictions, raw_outputs = model_biobert.predict(sample_texts)
# print("Predictions:", predictions)
# print("Raw Outputs:", raw_outputs)
# Decode label predictions
decoded_predictions = label_encoder.inverse_transform(predictions)
print("Decoded Predictions:", decoded_predictions)

#Another Example

sample_texts = ["  A total of @ patients were randomized ."] #correct lable is:RESULTS

## model_bert
print("### Predictions of fine-tuned 'bert-base-cased':\n")
predictions, raw_outputs = model_bert.predict(sample_texts)
# print("Predictions:", predictions)
# print("Raw Outputs:", raw_outputs)
# Decode label predictions
decoded_predictions = label_encoder.inverse_transform(predictions)
print("Decoded Predictions:", decoded_predictions)

## model_biobert
print("\n### Predictions of fine-tuned 'biobert-base-cased-v1.2':\n")
predictions, raw_outputs = model_biobert.predict(sample_texts)
# print("Predictions:", predictions)
# print("Raw Outputs:", raw_outputs)
# Decode label predictions
decoded_predictions = label_encoder.inverse_transform(predictions)
print("Decoded Predictions:", decoded_predictions)

"""# Data: BC5CDR (BioCreative V CDR corpus)

•	Task: Named Entity Recognition (NER)

•	Paper: Jiao Li et al. in BioCreative V CDR task corpus: a resource for chemical disease relation extraction. Database, 2016.

•	Details: BC5CDR corpus consists of 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions.

•	Data source : https://github.com/cambridgeltl/MTL-Bioinformatics-2016/raw/master/data/BC5CDR-IOB/


•	Labels: Chemical, Disease.

**Section 1: Setting up the GPU Environment**
"""

# Install necessary libraries
!pip install simpletransformers transformers pandas --quiet

import os
import pandas as pd
import requests
import logging
from simpletransformers.ner import NERModel, NERArgs
import torch

# Setup logging
logging.basicConfig(level=logging.DEBUG)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

"""**Section 2: Getting Data**"""

# Define the URLs and download the data
Base_URL = "https://github.com/cambridgeltl/MTL-Bioinformatics-2016/raw/master/data/BC5CDR-IOB/"
train_file = "train.tsv"
dev_file = "devel.tsv"
test_file = "test.tsv"

def download_file(url, filename):
    response = requests.get(url)
    with open(filename, 'wb') as f:
        f.write(response.content)
    print(f"{filename} downloaded.")

# Download files
download_file(Base_URL + train_file, "train.tsv")
download_file(Base_URL + dev_file, "devel.tsv")
download_file(Base_URL + test_file, "test.tsv")

# Data preprocessing

# Load the data into pandas DataFrames
def load_ner_data(file_path):
    """Load NER data from a file into a pandas DataFrame."""
    sentences = []
    sentence = []
    with open(file_path, 'r') as f:
        for line in f:
            if line.strip() == "":
                if sentence:
                    sentences.append(sentence)
                    sentence = []
            else:
                word, label = line.strip().split("\t")
                sentence.append((word, label))
    # Flatten into a DataFrame
    data = {"sentence_id": [], "words": [], "labels": []}
    for i, sentence in enumerate(sentences):
        for word, label in sentence:
            data["sentence_id"].append(i)
            data["words"].append(word)
            data["labels"].append(label)
    return pd.DataFrame(data)


# Load datasets
train_df = load_ner_data("train.tsv")
dev_df = load_ner_data("devel.tsv")
test_df = load_ner_data("test.tsv")
print("Example of preprocessd_train_data:\n",train_df)

"""**Section 3: Training and Testing the Model**"""

# Optional model configuration
model_args = NERArgs(
    num_train_epochs=1,             # Number of epochs
    train_batch_size=16,            # Batch size for training
    eval_batch_size=16,             # Batch size for evaluation
    overwrite_output_dir=True,      # Overwrite the output directory
    output_dir="./NER_Model_bert",  # Output directory
)

# Define labels for the NER model
custom_labels = train_df["labels"].unique().tolist()
print(f"Custom Labels: {custom_labels}")

# Create a NERModel: Pre-trained BERT model="bert-base-cased"
ner_model_bert = NERModel(
    "bert",                         # Use the BERT architecture
    "bert-base-cased",            # Pre-trained BERT model
    labels=custom_labels,  # labels for the NER model
    use_cuda=torch.cuda.is_available(),  # True: Use GPU if available
    args=model_args                 # Model arguments
)


# Train the model
ner_model_bert.train_model(train_df, eval_data=dev_df)

# Evaluate the model
result, model_outputs, preds_list = ner_model_bert.eval_model(test_df)

# Display evaluation results
print("Evaluation Results on fine-tuned 'bert-base-cased':", result)

# Create a NERModel: Pre-trained BERT model="biobert-base-cased-v1.2"
ner_model_biobert = NERModel(
    "bert",                         # Use the BERT architecture
    "dmis-lab/biobert-base-cased-v1.2",            # Pre-trained BERT model
    labels=custom_labels,  # labels for the NER model
    use_cuda=torch.cuda.is_available(),  # True: Use GPU if available
    args=model_args                 # Model arguments
)

# Train the model
ner_model_biobert.train_model(train_df, eval_data=dev_df)

# Evaluate the model
result, model_outputs, preds_list = ner_model_biobert.eval_model(test_df)

# Display evaluation results
print("Evaluation Results on fine-tuned 'biobert-base-cased-v1.2':", result)

"""**Section 4: Using the Model**"""

samples = ["Torsade de pointes ventricular tachycardia during low dose intermittent dobutamine treatment in a patient with dilated cardiomyopathy and congestive heart failure."]
##Disease Entities: Torsade de pointes ventricular tachycardia, Dilated cardiomyopathy, Congestive heart failure
##Chemical Entities: Dobutamine

print("### Predictions of fine-tuned 'bert-base-cased':\n")
predictions, _ = ner_model_bert.predict(samples)
for idx, sample in enumerate(samples):
  print('{}: '.format(idx))
  for word in predictions[idx]:
    print('{}'.format(word))

print("\n### Predictions of fine-tuned 'biobert-base-cased-v1.2':\n")
predictions, _ = ner_model_biobert.predict(samples)
for idx, sample in enumerate(samples):
  print('{}: '.format(idx))
  for word in predictions[idx]:
    print('{}'.format(word))

# Another Example

samples = ["Meloxicam induced liver toxicity."]
##Disease Entities: Liver toxicity
##Chemical Entities: Meloxicam

print("### Predictions of fine-tuned 'bert-base-cased':\n")
predictions, _ = ner_model_bert.predict(samples)
for idx, sample in enumerate(samples):
  print('{}: '.format(idx))
  for word in predictions[idx]:
    print('{}'.format(word))

print("\n### Predictions of fine-tuned 'biobert-base-cased-v1.2':\n")
predictions, _ = ner_model_biobert.predict(samples)
for idx, sample in enumerate(samples):
  print('{}: '.format(idx))
  for word in predictions[idx]:
    print('{}'.format(word))

# Another Example

samples = ["This first case of meloxicam related liver toxicity demonstrates the potential of this drug to induce hepatic damage."]
##Disease Entities: Liver toxicity, Hepatic damage
##Chemical Entities: Meloxicam

print("Predictions of fine-tuned 'bert-base-cased' :")
predictions, _ = ner_model_bert.predict(samples)
for idx, sample in enumerate(samples):
  print('{}: '.format(idx))
  for word in predictions[idx]:
    print('{}'.format(word))

print("Predictions of fine-tuned 'biobert-base-cased-v1.2' :")
predictions, _ = ner_model_biobert.predict(samples)
for idx, sample in enumerate(samples):
  print('{}: '.format(idx))
  for word in predictions[idx]:
    print('{}'.format(word))

